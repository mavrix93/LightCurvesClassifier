========================================
GAVO DC Software Reference Documentation
========================================

:Author: Markus Demleitner
:Email: gavo@ari.uni-heidelberg.de
:Date: |date|

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


Resource Descriptor Element Reference
=====================================

The following (XML) elements are defined for resource descriptors.  Some
elements are polymorous (Grammars, Cores).  See below for a reference
on the respective real elements known to the software.


Each element description gives a general introduction to the element's
use (complain if it's too technical; it's not unlikely that it is since
these texts are actually the defining classes' docstrings).

Within RDs, element properties that can (but need not) be written in XML
attributes, i.e., as a single string, are called "atomic".  Their types
are given in parentheses after the attribute name along with a default
value.

In general, items defaulted to Undefined are mandatory.  Failing to
give a value will result in an error at RD parse time.

Within RD XML documents, you can (almost always) give atomic children
either as XML attribute (``att="abc"``) or as child elements
(``<att>abc</abc>``).  Some of the "atomic" attributes actually contain
lists of items.  For those, you should normally write multiple child
elements (``<att>val1</att><att>val2</att>``), although sometimes it's
allowed to mash together the individual list items using a variety of
separators.

Here are some short words about the types you may encounter, together
with valid literals:

* boolean – these allow quite a number of literals; use ``True`` and
  ``False`` or ``yes`` and ``no`` and stick to your choice.
* unicode string – there may be additional syntactical limitations on
  those.  See the explanation
* integer – only decimal integer literals are allowed
* id reference – these are references to items within XML documents; all
  elements within RDs can have an ``id`` attribute, which can then be
  used as an id reference.  Additionally, you can reference elements
  in different RDs using <rd-id>#<id>.  Note that DaCHS does not support
  forward references (i.e., references to items lexically behind the
  referencing element).
* list of id references – Lists of id references.  The
  values could be mashed together with commas, but prefer multiple child
  elements.

There are also "Dict-like" attributes.  These are built from XML like::

  <d key="ab">val1</d>
  <d key="cd">val2</d>

In addition to key, other (possibly more descriptive) attributes for the
key within these mappings may also be allowed.  In special circumstances
(in particular with properties) it may be useful to add to a value::

  <property key="brokencols">ab,cd</property>
  <property key="brokencols" cumulative="True">,x</property>

will leave ``ab,cd,x`` in brokencols.

Many elements can also have "structure children".  These correspond to
compound things with attributes and possibly children of their own.
The name given at the start of each description is irrelevant to the
pure user; it's the attribute name you'd use when you have the
corresponding python objects.  For authoring XML, you use the name in
the following link; thus, the phrase "colRefs (contains Element
columnRef..." means you'd write ``<columnRef...>``.

Here are some guidelines as to the naming of the attributes:

* Attributes giving keys into dictionaries or similar (e.g., column
  names) should always be named ``key``
* Attributes giving references to some source of events or data
  should always be named ``source``, never "src" or similar
* Attributes referencing generic things should always be called
  ``ref``; of course, references to specific things like tables or
  services should indicate in their names what they are supposed to
  reference.

Also note that examples for the usage of almost everything mentioned
here can be found in in the `GAVO datacenter element reference`_.

.. _GAVO datacenter element reference: http://docs.g-vo.org/DaCHS/elemref.html

.. replaceWithResult getStructDocs(docStructure)


Active Tags
===========

The following tags are "active", which means that they do not directly
contribute to the RD parsed.  Instead they define, replay, or edit
streams of elements.

.. replaceWithResult getActiveTagDocs(docStructure)


Grammars Available
==================

The following elements are all grammar related.  All grammar elements
can occur in data descriptors.

.. replaceWithResult getGrammarDocs(docStructure)

Cores Available
===============

The following elements are related to cores.  All cores can only occur
toplevel, i.e. as direct children of resource descriptors.  Cores are
only useful with an id to make them referencable from services using
that core.

.. replaceWithResult getCoreDocs(docStructure)

Mixins
======

Mixins ensure a certain functionality on a table.  Typically, this is
used to provide certain guaranteed fields to particular cores.  For many
mixins, there are predefined procedures (both rowmaker applys and
grammar rowfilters) that should be used in grammars and/or rowmakers
feeding the tables mixing in a given mixin.

.. replaceWithResult getMixinDocs(docStructure, [
  "//products#table", "//scs#positions", "//scs#q3cindex",
  "//siap#bbox", "//siap#pgs", "//ssap#hcd", "//obscore#publish",
  "//obscore#publishSIAP", "//ssap#sdm-instance", "//ssap#mixc",
  "//obscore#publishSSAPHCD", "//epntap#table"])


Triggers
========

In the context of the GAVO DC, triggers are conditions on rows -- either
the raw rows emitted by grammars if they are used within grammars, or
the rows about to be shipped to a table if they are used within
tables.  Triggers may be used recursively, i.e., triggers may contain
more triggers.  Child triggers are normally or-ed together.

Currently, there is one useful top-level trigger, the `element
ignoreOn`_.  If an ignoreOn is triggered, the respective row is silently
dropped (actually, you ignoreOn has a bail attribute that allows you to
raise an error if the trigger is pulled; this is mainly for debugging).

The following triggers are defined:

.. replaceWithResult getTriggerDocs(docStructure)


Renderers Available
===================

The following renderers are available for allowing and URL creation.
The parameter style is relevant when adapting `condDescs`` or table
based cores to renderers:

* With clear, parameters are just handed through
* With form, suitable parameters are turned into vizier-like expressions
* With pql, suitable parameters are turned into their PQL counterparts,
  letting you specify ranges and such.

Unchecked renderers can be applied to any service and need not be
explicitely allowed by the service.

.. replaceWithResult getRendererDocs(docStructure)


Predefined Procedures
=====================

Procedures available for rowmaker apply
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter([
    "//procs#simpleSelect", "//procs#resolveObject",
    "//procs#mapValue", "//procs#fullQuery", "//procs#dictMap", 
    "//siap#computePGS",
    "//siap#computeBbox", "//siap#setMeta", "//ssap#setMeta",
    "//siap#getBandFromFilter", "//ssap#setMixcMeta",
    "//epntap#populate",
    ])(docStructure)


Procedures available for grammar rowfilters
'''''''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter([
    "//procs#expandComma", "//procs#expandDates",
    "//products#define", "//procs#expandIntegers",
    ])(docStructure)


Procedures available for datalink cores
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter(["//datalink#"+ s for s in 
    "fromStandardPubDID", 
    "trivialFormatter",
    "generateProduct",
    "sdm_genDesc",
    "sdm_genData",
    "fits_genDesc",
    "fits_makeWCSParams",
    "fits_makeHDUList",
    "fits_doWCSCutout",
    "fits_formatHDUs",
    "fits_makeLambdaSlice",
    "fits_makeLambdaMeta",
    ])(docStructure)


Predefined Streams
==================

Streams are recorded RD elements that can be replayed into resource
descriptors using the ``FEED`` active tag.  They do, however, support
macro expansion; if macros are expanded, you need to given them values
in the FEED element (as attributes).  What attributes are required
should be mentioned in the following descriptions for those predefined
streams within DaCHS that are intended for developer consumption.

Datalink-related Streams
''''''''''''''''''''''''

.. replaceWithResult getStreamsDoc(
    ['//datalink#'+id for id in ["sdm_plainfluxcalib", "sdm_cutout",
      "sdm_format", "fits_genKindPar", "fits_genPixelPar",
      "fits_standardDLFuncs", "fits_standardLambdaCutout"]])

Other Streams
'''''''''''''

.. replaceWithResult getStreamsDoc(
    ['//obscore#obscore-columns']
    +['//ssap#'+id for id in ["hcd_condDescs"]]
    +['//echelle#'+id for id in ["ssacols"]])


Metadata
========

Various elements support the setting of metadata through meta elements.
Metadata is used for conveying RMI-style metadata used in the VO
registry.  See [RMI]_ for an overview of those.  We use the keys given
in RMI, but there are some extensions discussed in `RMI-style
Metadata`_.

The other big use of meta information is for feeding templates.  Those
"local" keys should all start with an underscore.  You are basically
free to use those as you like and fetch them from your custom templates.
The predefined templates already have some meta items built in,
discussed in `Template Metadata`.

So, metadata is a key-value mapping.  Keys may be compound like in RMI,
i.e., they may consist of period-separated atoms, like
publisher.address.email.  There may be multiple items for each meta
key.

Meta inheritance
''''''''''''''''

When you query an element for metadata, it first sees if it has this
metadata.  If that is not the case, it will ask its meta parent.  This
usually is the embedding element.  It wil again delegate the request to
its parent, if it exists.  If there is no parent, configured defaults
are examined.  These are taken from rootDir/etc/defaultmeta, where they
are given as colon-separated key-value pairs, e.g.,

::

  publisher: The GAVO DC team
  publisherID: ivo://org.gavo.dc
  contact.name: GAVO Data Center Team
  contact.address: Moenchhofstrasse 12-14, D-69120 Heidelberg
  contact.email: gavo@ari.uni-heidelberg.de
  contact.telephone: ++49 6221 54 1837
  creator.name: GAVO Data Center
  creator.logo: http://vo.ari.uni-heidelberg.de/docs/GavoTiny.png

The effect is that you can give global titles, descriptions, etc.
in the RD but override them in services, tables, etc.  The configured
defaults let you specify meta items that are probably constant for
everything in your data center, though of course you can override these
in your RD elements, too.

In HTML templates, missing meta usually is not an error.  The
corresponding elements are just left empty.  In registry documents,
missing meta may be an error.

Meta formats
''''''''''''

Metadata must work in registry records as well as in HTML pages and
possibly in other places.  Thus, it should ideally be given in formats
that can be sensibly transformed into the various formats.

The GAVO DC software knows four input formats:

literal
  The textual content of the element will not be touched.  In
  HTML, it will end up in a div block of class literalmeta.

plain
  The textual content of the element will be whitespace-normalized,
  i.e., whitespace will be stripped from the start and the end,
  runs of blanks and tabs are replaced by a single blank, and empty
  lines translate into paragraphs.  In HTML, these blocks com in
  plainmeta div elements.  

rst
  The textual content of the element is interpreted as restructured
  text.  When requested as plain text, the restructured text itself is
  returned, in HTML, the standard docutils rendering is returned.

raw
  The textual content of the element is not touched.  It will be
  embedded into HTML directly.  You can use this, probably together
  with CDATA sections, to embed HTML -- the other formats should not
  contain anything special to HTML (i.e., they should be PCDATA in
  XML lingo).  While the software does not enforce this, raw content
  should not be used with RMI-type metadata.  Only use it for items that
  will not be rendered outside of HTML templates.


Macros in Meta Elements
'''''''''''''''''''''''

Macros will be expanded in meta items using the embedding element as
macro processors (i.e., you can use the macros defined by this element).


Typed Meta Elements
'''''''''''''''''''

While generally the DC software does not care what you put into meta
items and views them all as strings, certain elements are treated
specially.  The following meta element "types" are currently defined:

.. replaceWithResult getMetaTypeDocs()

While in XML meta, you can explicitely set the type of a meta item
(using the type attribute), it is more common to just use the inferred
type by the meta name.  Currently, the following meta keys imply types:

.. replaceWithResult getMetaTypedNames()

For type inference, only the last component of a meta path is
significant, i.e., both creator.logo and publisher.logo are of type
logo.


Metadata in Standard Renderers
''''''''''''''''''''''''''''''

Certain meta keys have a data center-internal interpretation, used
in renderers or writers of certain formats.  These keys should always
start with an underscore.  Among those are:

* _intro -- used by the standard HTML template for explanatory text
  above the seach form.
* _bottominfo -- used by the standard HTML template for explanatory text
  below the seach form.
* _copyright -- used by the standard HTML template for copyright-related
  information (there's also copyright in RMI; the one with the
  underscore is intended to be less formal).
* _related -- used in the standard HTML template for links to related
  services.  As listed above, this is a link, i.e., you can give a
  title attribute.
* _longdoc -- used by the service info renderer for an explanatory
  piece of text of arbitrary length.  This will usually be in
  reStructured text, and we recommend having the whole meta body in a
  CDATA section.
* _news -- news on the service.  See above at `Typed Meta Elements`_.
* _warning -- used by both the VOTable and the HTML table renderer.
  The content is rendered as some kind of warning.  Unfortunately,
  there is no standard how to do this in VOTables.  There is no
  telling if the info elements generated will show anywhere.
* _noresultwarning -- displayed by the default response template instead
  of an empty table (use it for things like "No Foobar data for your
  query")
* _type -- on Data instances, used by the VOTable writer to set the
  ``type`` attribute on ``RESOURCE`` elements (to either "results"
  or "meta").  Probably only useful internally.
* _plotOptions – typically set on services, this lets you configure
  the initial appearance of the javascript-based quick plot.  The value
  must be a javascript dictionary literal (like ``{"xselIndex": 2}``)
  unless you're trying CSS deviltry (which you could, using this meta;
  then again, if you can inject RDs, you probably don't need CSS attacks).
  Keys evaluated include:

  * xselIndex – 0-based index of the column plotted on the x-axis 
    (default: 0)
  * yselIndex – 0-based index of the column plotted on the y-axis
    (default: length of the column list; that's "histogram on y)
  * usingIndex – 0-based index of the plotting style selector.  For
    now, that's 0 for points and 1 for lines.


RMI-Style Metadata
''''''''''''''''''

For services (and other things) that are registred in the Registry, you
must give certain metadata items (and you can give more), where we take
their keys from [RMI]_.  We provide a `explanatory leaflet
<./data_checklist.pdf>`_ for data providers.  The most common keys --
used by the registry interface and in part by HTML and VOTable
renderers -- include:

* title -- this should in general be given seperately on the resource,
  each table, and each service.  In simple cases, though, you may get by
  by just having one global title on the resource and rely on metdata
  inheritance.
* shortName -- a string that should indicate what the service is in 16
  characters or less.
* creationDate -- Use ISO format with time, UTC only, like this: 
  2007-10-04T12:00:00Z
* subject -- as noted in the explanatory leaflet, these should be taken
  from the `IVOA Vocabulary Explorer
  <http://explicator.dcs.gla.ac.uk/WebVocabularyExplorer/>`_.
* copyright -- freetext copyright notice.
* source -- bibcodes will be expanded to ADS links here.
* referenceURL -- again, a link, so you can give a title for
  presentation purposes.  If you give no referenceURL, the service's
  info page will be used.
* dateUpdated -- an ISO date.  Do not set this.  This is determined
  from timestamps in DaCHS's state directory.  There is also
  datetimeUpdated that you would have to keep in sync with dateUpdated
  if you were to change it.
* creator.name -- this should be the name of the "author" of the data
  set.  See below for multiple creators.  If you set this, you may want
  to override creator.logo as well.
* content.type – one of Other, Archive, Bibliography, Catalog, 
  Journal, Library, Simulation, Survey, Transformation, Education, 
  Outreach, EPOResource, Animation, Artwork, Background, BasicData, 
  Historical, Photographic, Press, Organisation, Project, Registry –
  it's optional and we doubt its usefulness.
* facility -- no IVOA ids are supported here yet, but probably this
  should change.
* coverage -- see the special section
* service-specific metadata (for SIA, SCS, etc.) -- see the
  documentation of the respective cores.
* utype – tables (and possibly other items) can have utypes to signify
  their role in specific data models.  For tables, this utype gets
  exported to the tap_schema.

While you can set any of these in etc/defaultmeta.txt, the following items
are usually set there:

* publisher
* publisherID
* contact.name
* contact.address
* contact.email
* contact.telephone

The creator.name meta illustrates a pitfall with our metadata
definition.  Suppose you had more than one creator.  What you'd want is
a metadata structure like this::

  +-- creator -- name (Arthur)
  |
  +-- creator -- name (Berta)

However, if you write::

  creator.name: Arthur
  creator.name: Berta

or, equivalently::

  <meta name="creator.name">Arthur</meta>
  <meta name="creator.name">Berta</meta>

by the above rules, you'll get this::

  +-- creator -- name (Arthur)
         |
         +------ name (Berta)

i.e., one creator with two names.

To avoid this, make a new creator node in between, i.e., write::

  creator.name: Arthur
  creator:
  creator.name: Berta

In DaCHS resources, it's better to be explicit about the tree structure
(though you could write it like in metastream)::

  <meta name="creator">
    <meta name="name">Arthur</meta>
  </meta>
  <meta name="creator">
   <meta name="name">Berta</meta>
  </meta>

However, for creator.name specifically, it's highly likely that people
accept things like "Arthur; Berta" anyway, and so here it might be
better to disregard the tree structure issues entirely.

Actually, the DaCHS internal author table as used by the alternative
portal interprets one special notation::

  <author1>, <inits1> {; <authorn>, <initsn>}

That is, you should write authors lists like "Foo, X.; Bar, Q.; et al".


Coverage Metadata
'''''''''''''''''

Coverage metadata probably is the most complex piece of metadata, but
also potentially the most useful, since it would allow clients to
restrict querying to services known to contain relevant material.  So,
try to get it right.

Within DaCHS, coverage metadata uses the following keys:

* coverage.profile – an STC-S string giving the coverage of the service.
  These can become rather complex.  We implement several extensions to
  STC-S.  See also the `documentation for GAVO STC`_
* coverage.waveband – One of Radio, Millimeter, Infrared, Optical, UV,
  EUV, X-ray, Gamma-ray, and you can have multiple waveband
  specifications.  Note that you can provide much more detailed
  information on the covered spectral range as part of coverage.profile
  (but it's also much less likely that there is proper support for data
  there in registries and clients).
* coverage.regionOfRegard – in essence, the "pixel size" of the service in
  degrees.  If, for example, your service gives data on a lattice of
  sampling points, the typical distance of such points should be given
  here.  Leave out if this doesn't apply to your service.
* coverage.footprint – reserved; this will probably be filled in
  automatically by the software once we have a footprint standard and
  DaCHS implements it.

Here's an example for a service covering the large and small magellanic
clouds::

  <meta name="coverage">
    <meta name="profile">
      Union ICRS (
        Box 81 69.75 14 3.25
        Box 13 -73 9 2)</meta>
    <meta name="waveband">Optical</meta>
    <meta name="waveband">Infrared</meta>
    <meta name="regionOfRegard">0.02</meta>
  </meta>


.. _Documentation for GAVO STC: http://docs.g-vo.org/DaCHS/stc.html

Meta Stream Format
''''''''''''''''''

In serveral places, most notably in the ``defaultmeta.txt`` file and in
meta elements without a ``name`` attribute, you can give metadata as a
"meta stream".  This is just a sequence of lines containing pairs of
<meta key> and <meta value>.

In addition, there are comments, empty lines, and continuations.
Continuation lines work by ending a line with a backslash.  The
following line separator and all blanks and tabs following it are
then ignored.  Thus, the following two meta keys end up having identical
values::

  meta1: A contin\
    uation line needs \
      a blank if you wan\
  t one.
  meta2: A continuation line needs a blank if you want one

Note that whitespace behind a backslash prevents it from being a
continuation character.  That is, admittedly, a bit of a trap.

Other than their use as continuation characters, backslashes have no
special meaning within meta streams as such.  Within meta elements,
however, macros are expanded after continuation line processing if the
meta parent knows how to expand macros.  This lets you write things
like::

  <meta>
    creationDate: \metaString{authority.creationDate}
    managingOrg:ivo://\getConfig{ivoa}{authority}
  </meta>


Comments and empty lines are easy: Empty lines are allowed, and a
comment is a line with a hash (#) as its first non-whitespace
character.  Both constructs are ignored, and you can even continue
comments (though you should not).

Meta information can have a complex tree structure.  With meta streams,
you can build trees by referencing dotted meta identifiers.  If you
specify meta information for an item that already exists, a sibling will
be created.  Thus, after::

  creator.name: A. Author
  creator:
  creator.name: B. Buthor

there are two creator elements, each specifying a name meta.  For the
way creators are specified within VOResource, the following would be
wrong::

  creator.name: This is wrong.
  creator.name: and will not work

-- you would have a single creator meta with two name metas, which is
not allowed by VOResource.

If you write::

  contact.address: 7 Miner's Way, Behind the Seven Mountains
  contact.email: dwarfs@fairytale.fa

you have a single contact meta giving address and email.


Display Hints
=============

Display hints use an open vocabulary.  As you add value formatters, you can 
evaluate any display hint you like.  Display hints understood by the
built-in value formatters include:

checkmark
  in HTML tables, render this column as empty or checkmark depending on
  whether the value is false or true to python.

displayUnit
  use the value of this hint as the unit to display a value in.

humanTime
  display values as h:m:s.

nopreview
  if this key is present with any value, no HTML code to generate
  previews when mousing over a link will be generated.

sepChar
  a separation character for sexagesimal displays and the like.

sf
  "Significant figures" -- length of the mantissa for this column.
  Will probably be replaced by a column attribute analoguous to what
  VOTable does.

type
  a key that gives hints what to do with the column.  Values currently
  understood include:

  bar
    display a numeric value as a bar of length value pixels.

  bibcode
    display the value as a link to an ADS bibcode query.

  humanDate
    display a timestamp value or a real number in either yr (julian
    year), d (JD, or MJD if xtype is mjd), or s (unix timestamp) as 
    an ISO string.

  humanDay
    display a timestamp or date value as an ISO string without time.

  keephtml
    lets you include raw HTML.  In VOTables, tags are removed.

  product
    treats the value as a product key and expands it to a URL for the
    product (i.e., typically image).  This is defined in
    protocols.products.  This display hint is also used by, e.g., the tar
    format to identify which columns should contribute to the tar file.

  dms
    format a float as degree, minutes, seconds.

  simbadlink
    formats a column consisting of alpha and delta as a link to query
    simbad.  You can add a coneMins displayHint to specify the search
    radius.

  suppress
    do not automatically include this column in any table (e.g.,
    verbLevel-based column selection).

  hms
    force formatting of this column as a time (usually for RA).

  url
    makes value a link in HTML tables.  The anchor text will be the last
    element of the path part of the URL, or, if given, the value of the
    anchorText property of the column (which is for cases when you want
    a constant text like "Details").  If you need more control over the
    anchor text, use an outputField with a formatter.

  imageURL
    makes value the src of an image.  Add width to force a certain
    image size.

noxml
  if 'true' (exactly like this), do not include this column in VOTables.


Note that not any combination of display hints is correctly
interpreted.  The interpretation is greedy, and only one formatter at a
time attempts to interpret display hints.


Building Service Interfaces
===========================

Within DaCHS, an HTTP request is processed as follows:

1) The core is adapted to the renderer; this means that condDescs with
   buildFrom are converted to inputKeys according to the rules of the
   renderer.  The form renderer generates VizieR-like expressions,
   protocol renderers make PQL parameters, etc.  Also, onlyForRenderer
   and notForRenderer inputKeys are selected or deselected
2) From the core's inputTable, the service builds an input data
   descriptor (unless the service has an inputDD defined already, of
   course).  Most standard cores only take input from an input table's
   parameters (the exception being the computedCore), and hence the
   automatic inputDD will only have a parmaker.  The automatic
   inputDD will parse using ContextGrammar without a rowKey (i.e.,
   no rows will be produced).  The parmaker within the automatic inputDD
   parses the input with the default parsers and using the getHTTPPar
   rowmaker function.
3) The service will build the input table using its inputDD.  The input
   must be like nevow request.args, mapping each key to a sequence of
   strings.
4) The input table is passed to the core, which produces either a table,
   a data instance, or a pair of mime-type and content.
5) From the core result, an SvcResult is built.  This is relevant when
   the service has an outputTable defined, in which case the table
   structure is adapted if the input actually is a table.
6) The renderer formats the SvcResult according to its wishes.

There is special handling for the form renderer, which does its parsing
using nevow formal.  For it, the input table is built by just putting
the values of the dictionary nevow formal produces into the input table
params.


TBD: multiplicity, param values as defaults,


Table-based cores
'''''''''''''''''

You will usually deal with cores querying database tables – dbCore,
ssapCore, etc.  For these, there should not be a need to define an
inputDD; the one generated from the condDescs should work fine.

To create simple constraints, just ``buildFrom`` the columns queried::

  <condDesc buildFrom="myColumn"/>

(the names are resolved in the core's queried table).  This pattern has
the advantage that the concrete parameter style is adapted to the
renderer – in the web interface, there are vizier-like expressions, in
protocol interfaces, you get fields understanding expressions as in
SSAP's "PQL", plus in addition "structured parameters" (like FOO_MIN and
FOO_MAX) where applicable.

This will generate query fields that work against data as stored in the
database, with some exceptions (columns containing MJDs will, for
example, be turned into VizieR-like date expressions for web forms).
For protocol input, this is, in general, what you want.  In web forms,
you may want to customize the apprearance, for example, to adapt to
user's unit preferences.  For this latter use case, there is the
``inputUnit`` attribute::

    <condDesc>
      <inputKey original="minDist" inputUnit="arcsec"
        type="vexpr-float"
        onlyForRenderer="form"/>
      <inputKey original="minDist" 
        type="pql-float"
        notForRenderer="form"/>
    </condDesc>

Note how in this case we adapted the types of the input keys to provide
interfaces suitable to the various renderers.  For HTML forms, we
recommend one of

* vexpr-float
* vexpr-date (dates with timestamps in the database)
* vexpr-mjd (dates with MJD in the database)
* vexpr-string (though for those, frequently generating options is
  preferable, see below)

For protocol input, the types available are

* pql-int
* pql-float
* pql-string
* pql-date (where timestamps are in the database; MHD works fine with
  pql-float since date input is not really desirable for protocol input
  anyway).

Note that you can, of course, also keep the default types where that
provides a better interface.  Flag-like integers, for example, are
classic examples where giving the possible values is preferable to
allowing parameter expressions.

For object lists and similar, it is frequently desirable to give the
possible values (unless there are too many of those; these will be
translated to option lists in forms and to metadata items for protocol
services and hence be user visible)::

   <condDesc>
      <inputKey original="source">
        <values fromdb="source from plc.data"/>
      </inputKey>
    </condDesc>


All these generate the default SQL, which is equality (or set membership
for multiple values for a parameter).  To generate custom SQL, give a
phraseMaker, like this::

   <condDesc>
      <inputKey original="confirmed" multiplicity="single"/>
      <phraseMaker>
        <code>
          if inPars.get(inputKeys[0].name, False):
            yield "confirmed"
        </code>
      </phraseMaker>
    </condDesc>

PhraseMakers work like other code embedded in RDs (and thus may have
setup).  ``inPars`` gives a dictionary of the input parameters as parsed
by the inputDD according to multiplicity (or as delivered by nevow
formal – use the ``getHTTPPar`` rowmaker function if there can be input
with differing multiplicities).  ``inputKeys`` contains a sequence of
the condDesc's inputKeys.  By using their names as above, your code will
not break if the parameters are renamed.

PhraseMakers must yield zero or more SQL fragments; multiple SQL
fragments are joined in conjunctions (i.e., end up in ANDed conditions
in the WHERE clause).

Since you are dealing with raw SQL here, *never* include material from
inPars directly in the query strings you return – this would immediately
let people to SQL injections at least when the inputKey's type is
string.  Instead, use getSQLKey as in this example::

    <condDesc>
      <inputKey original="hdwl" multiplicity="single"/>
      <phraseMaker>
        <code>
          ik = inputKeys[0]
          destRE = "^%s\\.[0-9]*$"%inPars[ik.name]
          yield "%s ~ (%%(%s)s)"%(ik.name,
            base.getSQLKey("destRE", destRE, outPars))
        </code>
      </phraseMaker>
    </condDesc>

``getSQLKey`` takes a suggested name, a value and a dictionary, which
within phraseMakers always is ``outPars``. It will enter value with the
suggested name as key into outPars or change the suggested name if there
is a name clash.  The generated name will be returned, and that is what
is entered in the SQL statement.

The ``outPars`` dictionary is shared between all condDescs entering into
a query.  Hence, if you do anything with it except passing it to
``base.getSQLKey``, you're voiding your entire warranty.

Here's how to define a condDesc doing a full text search in a column::

  <condDesc>
    <inputKey original="source" description="Words from the catalog
      description, e.g., author names or title words."/>
    <phraseMaker>
      <code>
        yield ("to_tsvector('english', source)"
          " @@ plainto_tsquery('english', %%(%s)s)")%(
        base.getSQLKey("source", inPars["source"], outPars))
      </code>
    </phraseMaker>
  </condDesc>

Incidentally, this would go with an index definition like::

  <index columns="source" method="gin"
    >to_tsvector('english', source)</index>

For the HTML form interface, you can influence the widgets chosen by the
renderer to some extent.  To get an options list allowing multiple
selections, say::

    <condDesc>
      <inputKey original="carsfield" multipliticy="multiple">
        <values fromdb="carsfield from carsarcs.meta order by carsfield"/>
      </inputKey>
    </condDesc>

Use the ``showItems="n"`` attribute of inputKeys to determine how many
items in the selector are shown at one time.

For special effects, you can group inputKeys.  This will make them show
up under a common label and in a single line in HTML forms.  Here's an
example for a simple range selector::

  <condDesc>
    <inputKey name="el" type="text" tablehead="Element"/>

    <inputKey name="mfmin" tablehead="Min. Mass Fraction \item">
      <property name="cssClass">a_min</property>
    </inputKey>

    <inputKey name="mfmax" tablehead="Max. Mass Fraction \item">
      <property name="cssClass">a_max</property>
    </inputKey>

    <group name="mf">
      <description>Mass fraction of an element. You may leave out
        either upper or lower bound.</description>
      <property name="label">Mass Fraction between...</property>
      <property name="style">compact</property>
    </group>
  </condDesc>

You will probably want to style the result of this effort using the
``service`` element's ``customCSS`` property, maybe like this::

  <service...>
    <property name="customCSS">
      input.a_min {width: 5em}
      input.a_max {width: 5em}
      input.formkey_min {width: 6em!important}
      input.formkey_max {width: 6em!important}
      span.a_min:before { content:" between "; }
      span.a_max:before { content:" and "; }
      tr.mflegend td {
        padding-top: 0.5ex;
        padding-bottom: 0.5ex;
        border-bottom: 1px solid black;
      }
    </property>
  </service>

See also the entries on `multi-line input`_, `selecting input fields
with a widget`_, and `customizing generated SCS conditions`_.

.. _multi-line input: howDoI.html#get-a-multi-line-text-input-for-an-input-key
.. _selecting input fields with a widget: howDoI.html#make-an-input-widget-to-select-which-columns-appear-in-the-output-table
.. _customizing generated SCS conditions: howDoI.html#change-the-query-issued-on-scs-queries

TBD: Say something about required.  Do we even want to mention
widgetFactory?

Formatting the output
'''''''''''''''''''''

TBD


Regression Testing
==================

Introduction
''''''''''''

Things break – perhaps because someone foolishly dropped a database
table, because something happened in your upstream, because you changed
something or even because we changed the API (if that's not mentioned in
Changes, we owe you a beverage of your choice).  Given that, having
regression tests that you can easily run will really help your peace of
mind.

Therefore, DaCHS contains a framework for embedding regression tests in
resource descriptors.  Before we tell you how these work, some words of
advice, as writing useful regression tests is an art as much as
engineering.

*Don't overdo it.*  There's little point in checking all kinds of
functionality that only uses DaCHS code – we're running our tests before
committing into the repository, and of course before making a release.
If the services just use condDescs with buildFrom and one of the
standard renderers, there's little point in testing beyond a request
that tells you the database table is still there and contains something
resembling the data that should be there.

*Don't be over-confident.*  Just because it seems trivial doesn't
mean it cannot fail.  Whatever code there is in the service processing
of your RD, be it phrase makers, output field formatters, custom
render or data functions, not to mention custom renderers and cores,
deserves regression testing.

*Be specific.*  In choosing the queries you test against, try to find
something that won't change when data is added to your service, when
you add input keys or when doing similar maintenance-like this.  Change
will happen, and it's annoying to have to fix the regression test every
time the output might legitimately change.  This helps with the next point.

*Be pedantic.*  Do not accept failing regression tests, even if you
think you know why they're failing.  The real trick with useful testing
is to keep "normal" output minimal.  If you have to "manually" ignore
diagnostics, you're doing it wrong.  Also, sometimes tests may fail
"just once".  That's usually a sign of a race condition, and you should
*really* try to figure out what's going on.

*Make it fail first.*  It's surprisingly easy to write no-op tests
that run but won't fail when the assertion you think you're making is no
longer true.  So, when developing a test, assert something wrong first,
make sure there's some diagnostics, and only then assert what you really
expect.

*Be terse.*  While in unit tests it's good to test for maximally
specific properties so failing unit tests lead you on the right track as
fast as possible, in regression tests there's nothing wrong with
plastering a number of assertions into one test.  Regression tests
actually make requests to a web server, and these are comparatively
expensive.  The important thing here is that regression testing is fast
enough to let you run them every time you make a change.


Writing Regression Tests
''''''''''''''''''''''''

DaCHS' regression testing framework is organized a bit along the lines
of python's unittest and its predecessors, with some differences due to
the different scope.

So, tests are grouped into suites, where each suite is contained in a
regSuite_ element.  These have a (currently unused) title and a boolean
attribute ``sequential`` intended for when the tests contained must be
executed in the sequence specified and not in parallel.  It defaults to
false, which means the requests are made in random order and in
parallel, which speeds up the test runs and, in particular, will help
uncover race conditions.

On the other hand, if you're testing some sort of interaction across
requests (e.g., make an upload, see if it's there, remove it again),
this wouldn't work, and you must set `sequential="True"`.  Keep these
sequential suites as short as possible.  In tests within such suites
(and only there), you can pass information from one test to the
following one by adding attributes to ``self.followUp`` (which are
available as attributes of self in the next test).   If you need to
manipulate the next URL, it's at ``self.followUp.url.content_``.  For the
common case of a redirect to the url in the location header (or a child
thereof), there's the ``pointNextToLocation(child="")`` method of
regression tests.  In the tests that are manipulated like this, the URL
given in the RD should conventionally be ``overridden in the previous
test``.  Of course, additional parameters, httpMethods, etc, are still
applied in the manipulated url element.

Regression suites contain tests, represented in regTest_ elements.
These are procDefs (just like, e.g., rowmakery ``apply``), so you can
have setup code, and you could have a library of parametrizable regTests
procDefs that you'd then turn into regTests by setting their parameters.
We've not found that terribly useful so far, though.

You must given them a ``title``, which is used when reporting problems
with them.  Otherwise, the crucial children of these are ``url`` and, as
always with procDefs, ``code``.

Here are some hints on development:

1) Give the test you're just developing an id; at the GAVO DC, we're
   usually using cur; that way, we run variations of 
   ``gavo test rdId#cur``, and only the test in question is run.
2) After defining the url, just put an ``assert False`` into the test
   code.  Then run ``gavo test -Devidence.xml rdId#cur`` or similar.
   Then investigate ``evidence.xml`` (possibly after piping through
   ``xmlstarlet fo``) for stable and strong indicators that things are
   working.
3) If you get a BadCode for a test you're just writing, the message may
   not always be terribly helpful.  To see what's actually bugging
   python, run ``gavo --debug test ...`` and check dcInfos.

RegTest URLs
''''''''''''

The `url element`_ encapsulates all aspects of building the request.  In
the simplest case, you just can have a simple URL, in which case it
works as an attribute, like this::
  
  <regTest title="example" url="svc/form">
    ...

URLs without a scheme and a leading slash are interpreted relative to
the RD's root URL, so you'd usually just give the service id and the
renderer to be applied.  You can also specify root-relative and fully
specified URLs as described in the documentation of the `url element`_. 

White space in URLs is removed, which lets you break long URLs as
convenient.

You could have GET parameters in this URL, but that's inconvient due to
both XML and HTTP escaping.  So, if you want to pass parameters, just
give them as attributes to the element::

  <regTest title="example">
    <url RA="10" DEC="-42.3" SR="1" parSet="form">svc/form</url>

The ``parSet=form`` here sets up things such that processing for the
form renderer is performed – our form library nevow formal has some
hidden parameters that you don't want to repeat in every URL.  

To easily translate URLs taken from a browser's address bar or the form
renderer's result link, you can run ``gavo totesturl`` and paste the
URLs there.  Note that totesturl fails for values with embedded quotes,
takes only the first value of repeated parameters and is a over-quick
hack all around.  Patches are gratefully accepted.

The ``url`` element hence accepts arbitary attributes, which can be a
trap if you think you've given values to url's private attributes and
mistyped their names.  If uploads or authentication don't seem to
happen, check if your attribute ended up the in the URL (which is
displayed with the failure message) and fix the attribute name; 
most private url attributes start with ``http``.  If you really need
to pass a parameter named like one of url's private attributes, pass it
in the URL if you can.  If you can't because you're posting, spank us.
After teat, we'll work out something not too abominable .

If you have services requiring authentication, use url's ``httpAuthKey``
attribute.  We've introduced this to avoid having credentials in the RD,
which, after all, should reside in a version control system which may
be (and in the case of GAVO's data center is) public.  The attribute's
value is a key into the file ``~/.gavo/test.creds``, which contains, line
by line, this key, a username and a password, e.g.::

  svc1 testuser notASecret
  svc2 regtest NotASecretEither

A test using this would look like this::

  <regTest title="Authenticated user can see the light">
    <url httpAuthKey="svc1">svc1/qp/light.txt</url>
    <code>
      self.assertHTTPStatus(200)
    </code>
  </regTest>

By default, a test will perform a GET request.  To change this, set
the ``httpMethod`` attribute.  That's particularly important with
uploads (which must be POSTed).  

For uploads, the url element offers two facilities.  You can set a
request payload from a file using the ``postPayload`` attribute (the
path is interpreted relative to the resource directory), but it's much
more common to do a file upload like browsers do them.  Use the
``httpUpload`` element for this, as in::
  
  <url> <httpUpload name="UPLOAD"
    fileName="remote.txt">a,b,c</httpUpload> svc1/async </url>

(which will work as if the user had selected a file remote.txt
containing "a,b,c" in a browser with a file element named UPLOAD), or as
in::

  <url>
    <httpUpload name="UPLOAD" fileName="remote.vot"
      source="res/sample.regtest"/>
    svc1/async
  </url>

(which will upload the file referenced in ``source``, giving the remote
server the filename ``remote.vot``).  The ``fileName`` attribute is
optional.

Finally, you can pass arbitrary HTTP headers using the ``httpHeader``
element.  This has an attribute ``key``; the header's value is taken
from the element content, like this::

  <url postPayload="res/testData.regtest" httpMethod="POST">
    <httpHeader key="content-type">image/jpeg</httpHeader>
    >upload/custom</url>

RegTest Tests
'''''''''''''

Since regression tests are just procDefs, the actual assertions are
contained in the ``code`` child of the ``regTest``.  The code in there
sees the test itself in self, and it can access ``self.data`` (the
response content), ``self.headers`` (a sequence of header name, value
pairs; note that you should match the names case-insensitively here),
and ``self.status`` (the HTTP response code), as well as the URL
actually retrieved in ``self.url.httpURL`` (incidentally, that name is
right; the regression framework only supports http, and it's not
terribly likely that we'll change that).

You should probably only access those attributes in a pinch and instead
use the pre-defined assertions, which are methods on the test objects as
in pyunit – conventional assertions are clearer to read and less likely
to break if fixes to the regression test API become necessary.  If you
still want to have custom tests, raise AssertionErrors to indicate a
failure.

Here's a list of assertion methods defined right now:

.. replaceWithResult getRegtestAssertions(docStructure)


All of these are methods, so you would actually write
``self.assertHasStrings('a', 'b', 'c')`` in your test code (rather than
pass self explicitely.

When writing tests, you can, in addition, use assertions from python's
unittest TestCases (e.g., assertEqual and friends).  This is provided in
particular for use to check values in VOTables coming back from services
together with the ``getFirstVOTableRow`` method.

When writing tests, please note that, like all procDef's bodies, the test
code is macro-expanded by DaCHS.  This means that every backslash that
should be seen by python needs to be escaped itself (i.e., doubled).  An
escaped backslash in python thus is four backslashes in the RD.

Finally, here's a piece of ``.vimrc`` that inserts a ``regTest``
skeleton if you type ge in command mode (preferably at the start of
a line; you may need to fix the indentation if you're not indenting with
tabs.  We've thrown in a column skeleton on gn as well::

  augroup rd
    au!
    autocmd BufRead,BufNewFile *.rd set ts=2 tw=79
    au BufNewFile,BufRead *.rd map gn i<tab><tab><lt>column name="" type=""<CR><tab>unit="" ucd=""<CR>tablehead=""<CR>description=""<CR>verbLevel=""/><CR><ESC>5kf"a
    au BufNewFile,BufRead *.rd map ge i<tab><tab><lt>regTest title=""><CR><tab><lt>url><lt>/url><CR><lt>code><CR><lt>/code><CR><BS><lt>/regTest><ESC>4k
  augroup END


Running Tests
'''''''''''''

The first mode to run the regression tests is through ``gavo val``.  If
you give it a ``-t`` flag, it will collect regression tests from all the
RDs it touches and run them.  It will then output a brief report listing
the RDs that had failed tests for closer inspection.

It is recommended to run something like::
  
  ``gavo val -tv ALL``

before committing changes into your inputs repository.  That way,
regressions should be caught.

The tests are ran against the server described through the
``[web]serverURL`` config item.  In the recommended setup, this would be
a server started on your own development machine, which then would
actually test the changes you made.

There is also a dedicated gavo sub-command ``test`` for executing the
tests.  This is what you should be using for developing tests or
investigating failures flagged with ``gavo val``.  On its command line,
you can give on of an RD id or a cross-rd reference to a test suite,
or a cross-rd reference to an individual test.  For example,

::

  gavo test res1/q 
  gavo test res2/q#suite1 
  gavo test res2/q#test45

would run all the tests given in the RD ``res1/q``, the tests in
the regSuite with the ``id`` suite1 in ``res2/q``, and a test with
``id="test45`` in ``res2/q``, respectively.

To traverse inputs and run tests from all RDs found there, as well as
tests from the built-in RDs, run::

  gavo test ALL

``gavo test`` by default has a very terse output.  To see which tests
are failing and what they gave as reasons, run it with the '-v' option.

To debug failing regression tests (or maybe to come up with good things
to test for), use '-d', which dumps the server response of failing tests
to stdout.

In the recommended setup with a production server and a development
machine sharing a checkout of the same inputs, you can exercise
production server from the development machine by giving the ``-u``
option with what your production server has in its ``[web]serverURL``
configuration item.  So,

::

  gavo test -u http://production.example.com ALL

is what might help your night's sleep.


Examples
''''''''

Here are some examples how these constructs can be used.  First, a
simple test for string presence (which is often preferred even when
checking XML, as it's less likely to break on schema changes; these
usually count as noise in regression testing).  Also note how we have
escaped embedded XML fragments; an alternative to this shown below
is making the code a CDATA section::

  <regTest title="Info page looks ok" 
    url="siap/info">
    <code>
      self.assertHasStrings("SIAP Query", "siap.xml", "form", 
        "Other services", "SIZE&lt;/td>", "Verb. Level")
    </code>
  </regTest>

The next is a test with a "rooted" URL that's spanning lines, has
embedded parameters (not recommended), plus an assertion on binary
data::

  <regTest title="NV Maidanak product delivery"
    url="/getproduct/maidanak/data/Q2237p0305/Johnson_R/
      red_kk050001.fits.gz?siap=true">
    <code>
      self.assertHasStrings('\\x1f\\x8b\\x08\\x08')
    </code>
  </regTest>

This is how parameters should be passed into the request::

  <regTest title="NV Maidanak SIAP returns accref.">
    <url POS="340.12,3.3586" SIZE="0.1" INTERSECT="OVERLAPS" 
      _TDENC="True" _DBOPTIONS_LIMIT="10">siap/siap.xml</url>
    <code>
      self.assertHasStrings('&lt;TD>AZT 22')
    </code>
  </regTest>

Here's an example for a test with URL parameters and xpath assertions::

  <regTest title="NV Maidanak SIAP metadata query"
      url="siap/siap.xml?FORMAT=METADATA">
    <code>
      self.assertXpath("//v1:FIELD[@name='wcs_cdmatrix']", {
        "datatype": "double",
        "ucd": "VOX:WCS_CDMatrix",
        "arraysize": "*",
        "unit": "deg/pix"})
      self.assertXpath("//v1:INFO[@name='QUERY_STATUS']", {
        "value": "OK",
        None: "OK",})
      self.assertXpath("//v1:PARAM[@name='INPUT:POS']", {
        "datatype": "char",
        "ucd": "pos.eq",
        "unit": "deg"})
    </code>
  </regTest>

The following is a fairly complex example for a stateful suite doing
inline uploads (and simple tests)::

  <regSuite title="GAVO roster publication cycle" sequential="True">
    <regTest title="Complete record yields some credible output">
      <url httpAuthKey="gvo" parSet="form" httpMethod="POST">
        <httpUpload name="inFile" fileName="testing_ignore.rd"
          ><![CDATA[
            <resource schema="gvo">
              <meta name="description">x</meta>
              <meta name="title">A test service</meta>
              <meta name="creationDate">2010-04-26T11:45:00</meta>
              <meta name="subject">Testing</meta>
              <meta name="referenceURL">http://foo.bar</meta>
              <nullCore id="null"/>
              <service id="run" core="null" allowed="external">
                <meta name="shortName">u</meta>
                <publish render="external" sets="gavo">
                  <meta name="accessURL">http://foo/bar</meta>
                </publish></service></resource>
          ]]></httpUpload>upload/form</url>
      <code><![CDATA[
        self.assertHasStrings("#Published</th><td>1</td>")
      ]]></code>
    </regTest>

    <regTest title="Publication leaves traces on GAVO list" url="list/custom">
      <code>
        self.assertHasStrings(
          '"/gvo/data/testing_ignore/run/external">A test service')
      </code>
    </regTest>

    <regTest title="Unpublication yields some credible output">
      <url httpAuthKey="gvo" parSet="form" httpMethod="POST">
        <httpUpload name="inFile" fileName="testing_ignore.rd"
          ><![CDATA[
          <resource schema="gvo">
            <meta name="description">x</meta>
            <meta name="title">A test service</meta>
            <meta name="creationDate">2010-04-26T11:45:00</meta>
            <meta name="subject">Testing</meta>
            <meta name="referenceURL">http://foo.bar</meta>
            <service id="run" allowed="external">
              <nullCore/>
              <meta name="shortName">u</meta></service></resource>
          ]]></httpUpload>upload/form</url>
      <code><![CDATA[
        self.assertHasStrings("#Published</th><td>0</td>")
      ]]></code>
    </regTest>

    <regTest title="Unpublication leaves traces on GAVO list"
      url="list/custom">
      <code>
        self.assertLacksStrings(
          '"/gvo/data/testing_ignore/run/external">A test service')
      </code>
    </regTest>

  </regSuite>

If you still run SOAP services, here's one way to test them::

  <regTest id="soaptest" title="APFS SOAP returns something reasonable">
      <url postPayload="res/soapRequest.regtest" httpMethod="POST">
        <httpHeader key="SOAPAction">'"useService"'</httpHeader>
        <httpHeader key="content-type">text/xml</httpHeader
        >qall/soap/go</url>
      <code>
        self.assertHasStrings(
          '="xsd:date">2008-02-03Z&lt;/tns:isodate>', 
        '&lt;tns:raCio xsi:type="xsd:double">25.35')
      </code>
    </regTest>

– here, ``res/soapRequest.regtest`` would contain the request body that
you could, for example, extract from a tcpdump log.

.. _regSuite: #element-regsuite
.. _regTest: #element-regtest
.. _url element: #element-url




Datalink Cores
==============

[Note: Datalink specification in the IVOA is not yet finalized; it is
conceivable that the DaCHS Datalink API will have to change once it is]

Datalink is an IVOA protocol that allows associating various products
and artifacts with a data set id.  Classical examples for functionality
exposed via Datalink include cutouts, format conversions or
recalibrations done on the fly, and associating error or mask maps with
the actual data.

A central term for datalink is the pubDID, or publisher DID.  This is an
identifier assigned (essentially) by you that points to a concrete
dataset.  In DaCHS, datalink services always use pubDIDs as the values
of the datalink ID parameter.

Within DaCHS, you can write datalink services using a specialized type
of core.  Its function is twofold:

(1) when operated by the dlmeta renderer, it returns the access options 
    ("Datalink document")
(2) when operated by the dlget renderer, it performs some computation
    ("Processed data")

Function (1) is implemented by DaCHS code working on the metadata of the
Datalink core.  Function (2) requires custom code (or the assembly of
pre-provided building blocks).

A datalink core consists of 

* exactly one descriptor generator,
* zero or more data functions, generating and manipulating data
* zero or one formatters, formatting the generated and/or manipulated
  data
* zero or more meta makers, generating input parameter descriptions
  for data functions and any formatter present and/or related links

Here's how they work together in providing the Datalink functionality:

To generate the Datalink document, the descriptor generator is passed
the pubDID and is expected to return a ``datalink.ProductDescriptor``
instance (or None, in which case the datalink request will be rejected
by a 404).  In addition to attributes named after the columns of the
product table (and potentially other attributes added by deriving from
the base ProductDescriptor), it has an attribute ``data`` defaulting to
``None``, intended to be filled by the core's data generator on data
processing runs.

The descriptor is then passed, in turn, to the meta makers, which yield
``InputKey`` or ``LinkDef`` instances to describe the retrival options
for the product.  The combination of both is then formatted to a proper
Datalink document and returned, which concludes the processing of the
metadata request.

When a request for processed data comes in, the descriptor generator is
again used to make a product descriptor, and again the input keys are
updated as before.  They are then used to build the arguments structure
described by the input keys.  

If the context grammar succeeds, the data descriptor is passed to the
first data function together with the arguments parsed.  This must fill
out the ``data`` attribute of the descriptor or raise a ValidationError
for the PUBDID; leaving it as None results in a 500 server error.
Descriptor.data could an ``rsc.InMemoryTable`` (e.g., in SSAP) or a
products.Products instance, but as long as the other data functions and
the formatter agree on what it is, anything goes.  It will usually be
fed from a database, pixels in FITS files, or the like.

This object is then handed through all remaining data functions; these
change the data in place or create a new one as convenient and
manipulate ``descriptor.data`` accordingly.

Finally, the data enters the formatter, which actually generates the
output, returning a pair of mime type and string to be delivered.

It is a design descision which manipulations are done in the data
generator, which are in later filters, and which maybe only in the
formatter.  The advantage of filters is that they are more flexible and
can more easily be reused, while doing it things in the data generator
itself will usually be more efficient, sometimes much so (e.g., sums
being computed within a database rather than in a filter after all the
data had to go through the interface of the database).


Descriptors Generators
'''''''''''''''''''''''

Descriptor generators (see `element descriptorGenerator`_) are procedure
applications that see a pubDID value and are expected to return a
``datalink.ProductDescriptor`` instance, or something derived from it.  

In the end, this usually boils down to figuring out the value of accref
in the product table and using what's there to construct the descriptor
generatorr.  In the simplest case, the pubDID will be in DaCHS'
"standard" format (see the ``getStandardPubDID`` rowmaker function), in
which case the default descriptor generator works and you don't have to
specify anything.  You could manually insert that default by saying::
  
  <descriptorGenerator procDef="//datalink#fromStandardPubDID"/>

(but that would be silly since DaCHS already does this for you).  It's
functionality is equivalent to this::

  <descriptorGenerator>
    <code>
      return ProductDescriptor.fromAccref("/".join(pubDID.split("/")[4:]))
    </code>
  </descriptorGenerator>

– which might be a good place to start if you need to write your own
d.g., e.g., because you have some special logic to encode the accref in
the PubDID).

The default ``ProductDescriptor`` class exposes all the columns from the
products table, i.e., accref, accessPath, mime, owner, embargo, and
sourceTable, in addition to the pubDID itself.

A slightly more interesting example is provided by datalink for SSA,
where cutouts and similar is generated from spectra.  The actual
definition is in ``//datalink#sdm_genDesc``, but the gist of it is::

  <descriptorGenerator>
    <setup>
      <par key="ssaTD" description="Full reference (like path/rdname#id)
        to the SSA table the spectrum's PubDID can be found in."/>

      <code>
        from gavo import rsc
        from gavo import rscdef
        from gavo import svcs

        class SSADescriptor(ProductDescriptor):
          ssaRow = None

          @classmethod
          def fromSSARow(cls, ssaRow, paramDict):
            """returns a descriptor from a row in an ssa table and
            the params of that table.
            """
            paramDict.update(ssaRow)
            ssaRow = paramDict
            res = cls.fromAccref(ssaRow['accref'])
            res.ssaRow = ssaRow
            return res
      
        ssaTD = base.resolveCrossId("myres/q#mytable, rscdef.TableDef)
      </code>
    </setup>
    
    <code>
      with base.getTableConn() as conn:
        ssaTable = rsc.TableForDef(ssaTD, connection=conn)
        matchingRows = list(ssaTable.iterQuery(ssaTable.tableDef, 
          "ssa_pubdid=%(pubDID)s", {"pubDID": pubDID}))
        if not matchingRows:
          raise svcs.UnknownURI("No spectrum with pubDID %s known here"%
            pubDID)

        # the relevant metadata for all rows with the same PubDID should
        # be identical, and hence we can blindly take the first result.
        return SSADescriptor.fromSSARow(matchingRows[0],
          ssaTable.getParamDict())
    </code>
  </descriptorGenerator>

Note how we derive from ProductDescriptor to get something that metadata
makers can later consult to figure out the spectral extent, the
calibration status, etc., by combining a row from an SSA table and its
parameter dict and stuffing that into an attribute of the derived class.
Also, since SSA tables already contain a column containing PubDIDs, we
can treat them as opaque.

Incidentally, in this case you could stuff the entire code into the the
main code element, saving on the extra setup.  However, apart from a
minor speed benefit, keeping things like function or class definitions
in setup allows easier re-use of such definitions in procedure
applications and is therefore recommended.



Meta Makers
'''''''''''

Meta makers (see `element metaMaker`_) contain code that produces pieces
of service metadata from a data descriptor.  All meta makers belonging
to a service are unconditionally executed, and all must be generator
bodies (i.e., contain a yield statement).

Meta makers may yield input keys (``InputKey`` instances) and/or 
link definitions (``LinkDef`` instances).  The input
keys make up a service's interface in the usual way.

The classes usually required to build whatever meta makers return 
(InputKey, Values, Option, LinkDef) are available to the code as local
names.  

As usual, DaCHS structs – that's InputKey, Values, and Option here –
should not be constructed directly but only using the ``MS`` helper
(which is really an alias for base.makeStruct; it takes care that the
special postprocessing of DaCHS structures takes place).

Parameter Definitions
.....................

Hence, a meta maker that generates SSA cutout parameters could look like
this::

  <metaMaker>
    <setup>
      <code>
        parSTC = stc.parseQSTCS('SpectralInterval "LAMBDA_MIN" "LAMBDA_MAX"')
      </code>
    </setup>
    <code>
      for ik in genLimitKeys(MS(InputKey, name="LAMBDA",
        unit="m", stc=parSTC, ucd="em.wl", 
        description="Spectral cutout interval",
        values=MS(Values, 
          min=descriptor.ssaRow["ssa_specstart"],
          max=descriptor.ssaRow["ssa_specend"]))):
        yield ik
    </code>
  </metaMaker>

(something like this is part of the ``//datalink#sdm_cutout`` predefined
stream).

The example shows two general techniques for "physical" parameters.
For one, it defines an STC structure.  This is again the "quoted STC" as
discussed in `the DaCHS tutorial`_.  It is a good idea to create the STC
structure in the setup code since parsing STC-S can be relatively CPU 
intensive.  The STC structure resulting from should then be passed as
the ``stc`` keyword parameter to each input key mentioned in the STC
clause.

The second typical technique is the use of the ``genLimitKeys``
function.  This takes a "template" key specifying names, units, and
everything else that can be generically specified, and returns a
sequence of input keys for the limits (i.e., minimal and maximal value
for this).  You'll almost always want this when accepting floating-point
valued parameters, as matching these exactly is at least tricky and
rarely useful.

If the thing you are matching against actually is a column in a database
table, it is usually a good idea to build the input key from the column,
much like with the original mechanism in condition descriptors.  In
python code, this looks like this::

  <metaMaker>
    <code>
      baseIK = InputKey.fromColumn(
        rd.getById("orders").getColumnByName("ecorder")
       ).change(
        values=MS(Values, 
          min=descriptor.ssaRow["order_min"],
          max=descriptor.ssaRow["order_max"]))
      for ik in genLimitKeys(baseIk):
        yield ik
    </code>
  </metaMaker>

This takes input key metadata from the column ecorder in the table
orders.  The change method can take additional keyword/value pairs to
change further properties.

When publishing FITS cubes, you will usually use the
`//datalink#fits_makeWCSParams`_ meta maker; it accepts similar QSTCS
specifications as well.  To find out what parameter names the individual
axes are mapped to, first use makeWCSParams without the STC metadata::

   <service id="d" allowed="dlmeta,dlget,form">
    <datalinkCore>
      <descriptorGenerator procDef="//datalink#fits_genDesc"/>
      <metaMaker procDef="//datalink#fits_makeWCSParams"/>
    </datalinkCore>
  </service>

Then have a look at the metadata produced for a file.  Unless you did
something special, to do that you can just take the accref of a file
from the table containing the products; if the source table was
``mlqso.cubes``, you could figure one out via::

  select accref from dc.products where sourcetable='mlqso.cubes' limit 1

(talk to postgres directly for this query, dc.products is not available
via TAP).

The standard pubDID (as assigned using the ``getStandardPubDID``
rowmaker function) uses your datacenter authority (as configured in
/etc/gavo.rd, when you forget it you can also figure it out by using
``gavo config ivoa authority``) and this accref like this::

  ivo://<authority>/~?<accref>

Hence, to retrieve the datalink document for
``mlqso/data/FBQ0951_data.fits`` on the server dc.g-vo.org using the
datalink renderer on the ``mlqso/q/d`` service, you'd write::

  curl -DID=ivo://org.gavo.dc/~?mlqso/data/FBQ0951_data.fits \
    http://dc.g-vo.org/mlqso/q/d/dlmeta | xmlstarlet fo

(of course, ``xmlstarlet`` isn't actually necessary, and you can use
``wget`` if you want, but you get the idea).

In there you'll see the parameter names for the axes, e.g.,::

  $ curl -s -FID="ivo://org.gavo.dc/~?mlqso/data/FBQ0951_data.fits" \
  >> http://dc.g-vo.org/mlqso/q/d/dlmeta \
  >> | xmlstarlet sel -N v=http://www.ivoa.net/xml/VOTable/v1.2 -T \
  >> -t -m "//v:PARAM" -v "@name" -nl
  serviceAccessURL
  ID
  DEC_MIN
  DEC_MAX
  RA_MIN
  RA_MAX
  WAVELEN_1_MIN
  WAVELEN_1_MAX

If the image is calibrated using a catalog on ICRS, with the wavelength
given as measured, change the ``fits_makeWCSParams`` call to::

  <metaMaker procDef="//datalink#fits_makeWCSParams>
    <setup>
      <bind key="stcs"
        >('PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" "DEC_MAX"\n'
          'SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"')
       </bind>
    </setup>
  </metaMaker>

The effect should be a group like::

    <GROUP utype="stc:CatalogEntryLocation">
      <PARAM arraysize="*" datatype="char" 
        name="CoordFlavor" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordFlavor" value="SPHERICAL"/>
      <PARAM arraysize="*" datatype="char" 
        name="CoordRefFrame" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordRefFrame" value="ICRS"/>
      <PARAM arraysize="*" datatype="char" 
        name="ReferencePosition" 
        utype="stc:AstroCoordSystem.SpectralFrame.ReferencePosition" 
        value="TOPOCENTER"/>
      <PARAM arraysize="*" datatype="char" name="URI" 
        utype="stc:DataModel.URI" 
        value="http://www.ivoa.net/xml/STC/stc-v1.30.xsd"/>
      <PARAMref ref="apausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C1"/>
      <PARAMref ref="aedwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C2"/>
      <PARAMref ref="asausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C1"/>
      <PARAMref ref="ahgwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C2"/>
      <PARAMref ref="ahiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.HiLimit"/>
      <PARAMref ref="aeiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.LoLimit"/>
    </GROUP>

All this is explained in [VOTSTC]_.


Link Definitions
.................

When returning link definitions, the tricky part mostly is to come up
with the URLs.  Use the ``makeAbsoluteURL`` rowmaker function to make
them from relative URLs; the rest just depends on your URL scheme.  An
example could look like this::

  <metaMaker>
    <code>
      yield LinkDef(descriptor.PubDID,
        makeAbsoluteURL("get/"+descriptor.accref[:-5]+".err.fits"),
        contentType="image/fits", semantics="errors",
        description="Errors for this dataset")
      yield LinkDef(descriptor.pubDID,
        "http://foo.bar/raw/"+descriptor.accref.split("/")[-1],
        contentType="image/fits", semantics="raw",
        description="Un-flatfielded, uncalibrated source data")
    </code>
  </metaMaker>

In addition to the pubDID and the access URL, LinkDefs accept keyword
arguments for the columns of the ``//datalink#dlresponse`` table.  At
the time of this writing, these include:

serviceType
  This is a URI saying what kind of a service is behind accessURL if it
  is a service.  You'll not usually want to set this in links you give.
  The exception is when you're pointing to some standard service that
  exposes the data that's being talked about, too.  As of this writing,
  it is probably most relevant for SSA services, in which case you'd put
  ``ivo://ivoa.net/std/SSA`` here.  Otherwise, this field is used to
  generate a link to a descriptor of the services' dlget endpoint, but
  that's done automatically by DaCHS.
errorMessage
  If your code cannot make a link and wants to communicate that to a
  client, you leave accessURL empty (None) and set a message here.
description
  A human-readable short information what's behind the link
semantics
  A term from a controlled-vocabulary describing what's behind the link
  (see below)
contentType
  An (advisory) mime type of whatever accessURL points to.  Please make
  sure it's consistent with what the server actually returns if the
  protocol used by accessURL supports that.
contentLength
  The size of the resource at accessURL, in bytes.

All auxillary data defaults to None if not given, and it's legal to
leave it at that.

You can inspect the definition of the datalinks table active in your
system by saying::

  gavo admin dumpDF //datalink | less

(the table definition is right at the top).

The vocabulary for the semantics value is not finalized yet.  Complain
to the DaCHS maintainers that this sentence still is here.
At the time of writing, the following terms are in discussion:

science
  Associated datasets containing science data, possibly derived or
  further reduced data; an example would be a merged spectrum from
  Echelle orders.
qa
  Files containing error estimates or the like.
calibration
  Datasets used for reduction of the current dataset.
preview
  Scaled-down or rendered versions of the datasets; this could include a
  small plot for spectra, say.
info
  Erm.  Frankly, I don't know.
self
  The data set itself.  DaCHS generates such a link automatically to a
  dlget giving just the PubDID if the renderer is enabled on the service.  
  If this is a problem for you, let us know and we'll hack something 
  that lets you suppress it.
source
  A predecessor data set, e.g., uncalibrated or less calibrated data
access
  A service that allows access to parts of the dataset, or to processed
  versions of it.  DaCHS automatically makes a link with this semantics
  for the dlget endpoint if the ``dlget`` renderer is enabled on the
  service.
auxiliary
  Basically, a "none of the above" option.  You'll probably want to
  use the description column to help clients figure out what it is.

Data links frequently should expose some data that's not in the product
table (e.g., because you don't want the error files to show up there).
In such cases, a good pattern is to put a static renderer next to the
datalink renderer and then write a meta maker like this::

  <service id="d" allowed="dlget,dlmeta,static">
    <property name="staticData">data/errors</property>

    <datalinkCore>
      ...
      <metaMaker>
        <code>
          stem = descriptor.accref.split("/")[-1].split("_")[0]
          yield LinkDef(descriptor.pubDID, makeAbsoluteURL(
            "\rdId/d/static/%s_err.fits"%stem),
            contentType="image/fits", semantics="qa")
        </code>
      </metaMaker>
    </datalinkCore>
  </service>

Note, however, that the static renderer does not enforce any access
control.  That means that embargoed files must never be within the
staticData (or they are not embargoed any more...)


Metadata Error Messages
'''''''''''''''''''''''

Both description generators and meta makers can return (or yield, in the
case of meta makers) error messages instead of either a descriptor or a
link definition.  This allows more fine-tuned control over the messages
generated than raising an exception.

Error messages are constructed using class functions of
``DatalinkError``, which is visible to both procedure types.  The class
function names correspond to the message types defined in the datalink
spec and match the semantics given there:

* AuthenticationError
* AuthorizationError
* NotFoundError
* UsageError
* TransientError
* FatalError
* Error

Thus, a descriptor generator could look like this::

  <descriptorGenerator>
    <setup>
      <code>
        class MyCustomDescriptor(ProductDescriptor):
          ...
      </code>
    </setup>
    <code>
      with base.getTableConn() as conn:
        matchingRows = list(conn.queryToDicts(
          "select physPath from schema.myTable where pub_did=%(pubDID)s",
          locals()))
        if not matchingRows:
          return DatalinkError.NotFoundError(pubDID,
            "No dataset with this pubDID known here")
        return MyCustomDescriptor.fromFile(matchingRows[0]["physPath"])
    </code>
  </descriptorGenerator>



Data Functions
''''''''''''''

Data functions (see `element dataFunction`_) generate or manipulate
data.  They see the descriptor and the arguments, parsed according to
the input keys produced by the meta makers, where the descriptor's
``data`` attribute is filled out by the first data function called (the
"generating data function").

As described above, DaCHS does not enforce anything on the ``data``
attribute other than that it's not None after the first data function
has run.  It is the RD author's responsibility to make sure that all
data functions in a given datalink core agree on what ``data`` is.

All code in a request for processed data is also passed the input
parameters as processed by the context grammar.  Hence, the code can
rely on whatever contract is implicit in the context grammar, but not
more.  In particular, a datalink core has no way of knowing what data
functions expects which parameters.  If no value for a parameter was
provided on input, the corresponding value is None but a data function
using it still is called.

An example for a generating data function is ``//datalink#generateProduct``,
which may be convenient when the manipulations operate on plain local files;
it basically looks like this::

  <dataFunction>
    <code>
      descriptor.data = products.getProductForRAccref(descriptor.accref)
    </code>
  </dataFunction>

(the actual implementation lets you require certain mime types and is
therefore a bit more complicated).

Another generating data function, this time creating a Data instance
containing a spectral data model-compliant structure, is in
``//datalink#sdm_genData`` and looks essentially like this::

  <dataFunction>
    <code>
      from gavo import rscdef
      from gavo.protocols import sdm
      builder = base.resolveCrossId(
        "flashheros/q#buildsdm, rscdef.DataDescriptor)
      descriptor.data = sdm.makeSDMDataForSSARow(descriptor.ssaRow, builder)
    </code>
  </dataFunction>

More on this will be discussed in our section on SDM support.

Filtering data functions should always come with a corresponding
metaMaker.  As an example, continuing the spectral cutout example above,
is again in ``//datalink#sdm_cutout``.  It simply looks like this::

  <dataFunction>
    <code>
      if not args.get("LAMBDA_MIN") and not args.get("LAMBDA_MAX"):
        return

      from gavo.protocols import sdm
      sdm.mangle_cutout(
        descriptor.data.getPrimaryTable(),
        args["LAMBDA_MIN"] or -1, args["LAMBDA_MAX"] or 1e308)
    </code>
  </dataFunction>

There are situations in which a data function must shortcut, mostly
because it is doing something other than just "pushing on"
descriptor.data.  Examples include preview producers or a data function
that returns the a FITS header.  For cases like this, data functions can
raise one of DeliverNow (which means ``descriptor.data`` must be
something servable, see `Data Formatters`_ and causes that to be
immediately served) or FormatNow (which immediately goes to the data
formatter; this is less useful).

Here's an example for FormatNow; a similar thing is contained in the
STREAM ``//datalink#fits_genKindPar``::

  <dataFunction>
    <setup>
      <code>
        from gavo.utils import fitstools
      </code>
    </setup>
    <code>
      if args["KIND"]=="HEADER":
        descriptor.data = ("application/fits-header", 
          fitstools.serializeHeader(descriptor.data[0].header))
        raise DeliverNow()
    </code>
  </dataFunction>



Data Formatters
'''''''''''''''

Data formatters (see `element dataFormatter`_) take a descriptor's data
attribute and build something serveable out of it.  Datalink cores do
not absolutely need one; the default is to return ``descriptor.data``
(the ``//datalink#trivialFormatter``, which might be fine if that data is
serveable itself).

What is serveable?  The easiest thing to come up with is a pair of
content type and data in byte strings; if ``descriptor.data`` is a Table
or Data instance, the following could work::

  <dataFormatter>
    <code>
      from gavo import formats

      return "text/plain", formats.getAsText(descriptor.data)
    </code>
  </dataFormatter>
 

Another example is an excerpt from ``//datalink#sdm_cutout``::

  <dataFormatter>
    <code>
      from gavo.protocols import sdm

      if len(descriptor.data.getPrimaryTable().rows)==0:
        raise base.ValidationError("Spectrum is empty.", "(various)")

      return sdm.formatSDMData(descriptor.data, args["FORMAT"])
    </code>
  </dataFormatter>

(this goes together with a metaMaker for an input key describing
FORMAT).

An alternative is to return something that has a ``renderHTTP(ctx)``
method that works in nevow.  This is true for the Product instances that
``//datalink#generateProduct`` generates, for example.  You can also
write something yourself by inheriting from
protocols.products.ProductBase and overriding its iterData method.

If you don't inherit from ProductBase, take care that this renderHTTP
runs in the main server loop.  If it blocks, the server blocks, so make
sure that this doesn't happen.  The conventional way would be to return,
from the renderHTTP method, some twisted producer.  Non-Product nevow
resources will also not work with asynchronous datalink at this point.


Registry Matters
''''''''''''''''

You can publish the metadata generating endpoint on your service by
saying ``<publish render="dlmeta" sets="ivo_managed"/>``.  However, that
is not recommended, as it clutters the registry with services that
are not really usable after discovery.

An alternative is to add the capability to the service that houses the
discovered datasets.  TODO: Tell people how :-)


Datalink and Obscore
''''''''''''''''''''

In particular for larger datasets like cubes, it is rude to put the
entire dataset into an obscore table.  Although obscore gives expected
download sizes, clients nevertheless do not usually expecte to have to
retrieve several gigabytes or even terabytes of data when dereferencing
an obscore access URL.

Instead, the access URL in the obscore table can point to a datalink
service.  There are various ways to effect this, but the recommended one
is to precompute datalink URLs in the embedding table and then pass
that column to obscore.  This entails defining a suitable column, which
could look like this::

  <column name="dlurl" type="text"
    ucd="meta.ref.url"
    tablehead="DL"
    description="URL of a datalink document for this dataset"
    verbLevel="1" displayHint="type=url"/>

Then fill this column in the rowmaker::

  <var key="obs_id">\standardPubDID</var>
    <map key="dlurl">makeAbsoluteURL(
      "\rdId/*dl*/dlmeta?ID="+urllib.quote(@obs_id))</map>

– you'll need to change ``*dl*`` to the id of your datalink service.
It is true that this stores quite a bit of stuff in the database that
could be computed at runtime.  However, even when you have 1e5 datasets,
we'd be only talking of savings of the order of 10 MB, at the cost of a
seriously ugly obscore expression and reduced debuggability.  Having
said that, it wouldn't be too hard to build these URLs in the obscore
mixin.  With these column, however, you can just state::

  <mixin
    accessURL="dlurl"
    size="10"
    mime="'application/x-votable+xml;content=datalink'"
    ... (all the other stuff) ...
    >//obscore#publish</mixin>

This says that what's coming back is going to be about 10k; it's hard to
predict the exact size of a datalink response, and there's no need to
sweat things for a couple of k more or less.  The mime type given here
is defined by Datalink exactly this purpose.  This is non-negotiable if
you want clients to understand your data.


Datalink Examples
'''''''''''''''''

FITS cutout service
...................

A plain FITS cutout service is assembled like this::

  <service id="dl">
    <meta name="title">Generic FITS datalink service</meta>
    <datalinkCore>
      <descriptorGenerator procDef="//datalink#fits_genDesc"/>
      <metaMaker procDef="//datalink#fits_makeWCSParams"/>
      <dataFunction procDef="//datalink#fits_makeHDUList"/>
      <dataFunction procDef="//datalink#fits_doWCSCutout"/>
      <FEED source="//datalink#fits_genKindPar"/>
      <dataFormatter procDef="//datalink#fits_formatHDUs"/>
    </datalinkCore>
  </service>

This works for all FITS files in the products table and has no usable
STC metadata.  Good datalink services do better; by giving more
metadata, you of course commit to certain FITS structures, which means
that you should restrict to only those files that actually match your
assumptions.  The easiest way to do this is to structure your input
directories accordingly and then filter early by using fits_getDesc's
``accrefStart`` parameter.  The STC declaration was already discussed
above, and so a more realistic datalink service might look like this::

  <service id="dl">
    <meta name="title">Datalink service for califa cubes</meta>
    <datalinkCore>
      <descriptorGenerator procDef="//datalink#fits_genDesc">
        <bind key="accrefStart">califa/data/cubes"</bind>
      </descriptorGenerator>
      <metaMaker procDef="//datalink#fits_makeWCSParams">
        <bind key="stcs"
          >('PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" "DEC_MAX"\n'
            'SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"')
         </bind>
        </metaMaker>
      <dataFunction procDef="//datalink#fits_makeHDUList"/>
      <dataFunction procDef="//datalink#fits_doWCSCutout"/>
      <FEED source="//datalink#fits_genKindPar"/>
      <dataFormatter procDef="//datalink#fits_formatHDUs"/>
    </datalinkCore>
  </service>

All of this (and potentially more as we expand the manipulation options)
is included with the `//datalink#fits_standardDLFuncs`_ STREAM; the above
specification then boils down to::

  <service id="dl">
    <meta name="title">Shortened FITS datalink service.</meta>
    <datalinkCore>
      <FEED source="//datalink#fits_standardDLFuncs"
        stcs='PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" 
          "DEC_MAX"\n
            SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"'
        accrefStart="califa/data/cubes"/>
    </datalinkCore>
  </service>


TODO: Custom function, return link to error file.


SSAP auxillary datalink
.......................

Another use for datalink cores in DaCHS is for server-side processing of
spectra.  A typical service there looks like this::

  <service id="ssaaux" allowed="dlmeta,dlget">
    <meta name="title">Datalink service to retrieve individual spectra</meta>
    <datalinkCore>
      <descriptorGenerator procDef="//datalink#sdm_genDesc">
        <bind name="ssaTD">"\rdId#slitspectra"</bind>
      </descriptorGenerator>
      <dataFunction procDef="//datalink#sdm_genData">
        <bind name="builder">"\rdId#get_slitcomponent"</bind>
      </dataFunction>
      <FEED source="//datalink#sdm_plainfluxcalib"/>
      <FEED source="//datalink#sdm_cutout"/>
      <FEED source="//datalink#sdm_format"/>
    </datalinkCore>
  </service>

For SDM processing, the descriptor contains the SSA row, and thus
the descriptor generator needs to know the SSA table that keeps the
spectra to be processed.  It is here identified using a full reference
(i.e., including the RD id) to the table definition, in the ssaTD
parameter of ``sdm_genData``; this must of course match whatever is the
``queriedTable`` of the core of the SSA service that refers to this
datalink service. 

The ``sdm_genData`` data function should again cover most uses of this.
Its parameter, however, is somewhat more involved.  The data attribute
of SDM descriptors contains SDM compliant data items, which need to be
created using appropriate RD ``data`` elements.  Such a ``data`` element
needs to be referenced in the ``builder`` parameter of ``sdm_genData``,
again including the RD in the reference.

To define this ``builder``, you first need to define an instance table.
The columns that are in there depend on your data.  In the simplest
case, the ``//ssap#sdm-instance`` mixin is sufficient and bring columns
name ``flux`` and ``spectral``. Here's how you'd add flux errors if you
needed to::

  <table id="instance" onDisk="False">
    <mixin ssaTable="slitspectra"
      spectralDescription="Wavelength"
      fluxDescription="Flux"
      >//ssap#sdm-instance</mixin>

    <meta name="description">A spectrum from a slit spectrum obtained
      for systems of quasars and lensing galaxies.  See 
      ivo://org.gavo.dc/mlqso/q/q</meta>

    <column name="fluxerror" ucd="stat.error;phot.flux.density;em.wl"/>
  </table>

What's referenced in the datalink core's builder data function then is a
``data`` element that builds this table.  Here's one that fills the table
from the database::

  <data id="get_slitcomponent">
    <!-- datamaker to pull spectra values out of the database -->
    <embeddedGrammar>
      <iterator>
        <code>
          obsId = self.sourceToken["accref"].split("/")[-1]
          with base.getTableConn() as conn:
            for row in conn.queryToDicts(
                "SELECT lambda as spectral, flux, error as fluxerror"
                " WHERE obsId=%(obsid)s ORDER BY lambda"):
              yield row
        </code>
      </iterator>
     </embeddedGrammar>
    <make table="instance">
       <parmaker>
         <apply procDef="//ssap#feedSSAToSDM"/>
       </parmaker>
    </make>
  </data>

The ``parmaker`` with the ``//ssap#feedSSAToSDM`` call is generic.  You
will in general have to write custom code for the embedded grammar; just
yield rows matching the instance table.


Product Previews
================

DaCHS has built-in machinery to generate previews from normal, 2D FITS
and JPEG files, where these are versions of the original dataset scaled
to be about 200 pixels in width, delivered as JPEG files.  These
previews are shown on mousing over product links in the web interface,
and they turn up as preview links in datalink interfaces.  This
also generates previews for cutouts.

For any other sort of data, DaCHS does not automatically generate
previews.  To still provide previews – which is highly recommended –
there is a framework allowing you to compute and serve out custom
previews.  This is based on the ``preview`` and ``preview_mime`` columns
which are usually set using parameters in ``//products#define``.

You could use external previews by having http (or ftp) URLs, which
could look like this::

  <rowfilter procDef="//products#define">
    ...
    <bind key="preview">("http://example.org/previews/"
      +"/".join(\inputRelativePath.split("/")[2:]))</bind>
    <bind key="preview_mime">"image/jpeg"/bind>
  </rowfilter>

(this assumes takes away to path elements from the relative paths, which
typically reproduces an external hierachy).  If you need to do more
complex manipulations, you can have a custom rowfilter, maybe like
this if you have both FITS files (for which you want DaCHS' default
behaviour selected with ``AUTO``) and ``.complex`` files with some
external preview::

  <rowfilter name="make_preview_paths">
    <code>
      srcName = os.path.basename(rowIter.sourceToken)
      if srcName.endswith(".fits"):
        row["preview"] = 'AUTO'
        row["preview_mime"] = None
      else:
        row["preview"] = ('http://example.com/previews'
          +os.path.splitext(srcName)[0]+"-preview.jpeg")
        row["preview_mime"] = 'image/jpeg'
      yield row
    </code>
  </rowfilter>
  <rowfilter procDef="//products#define">
    ...
    <bind key="preview">@preview</bind>
    <bind key="preview_mime">@preview_mime</bind>
  </rowfilter>

More commonly, however, you'll have local previews.  If they already
exist, use a static renderer and enter full local URLs as above.

If you don't have pre-computed previews, let DaCHS handle them for you.
You need to do three things:

a) define where the preview files are.  This happens via a
   ``previewDir`` property on the importing data descriptor, like this::

    <data id="import">
      <property key="previewDir">previews</property>
      ...
b) say that the previews are standard DaCHS generated in the
   ``//products#define`` rowfilter.  The main thing you have to
   decide here is the MIME type of the previews you're generating
   (i.e., use the ``standardPreviewPath`` macro unless you know what
   you are doing)::

      <rowfilter procDef="//products#define">
        <bind name="table">"\schema.data"</bind>
        <bind name="mime">"image/fits"</bind>
        <bind name="preview_mime">"image/jpeg"</bind>
        <bind name="preview">\standardPreviewPath</bind>
      </rowfilter>
    
   
c) actually compute the previews.  This is usually not defined in the RD
   but rather using DaCHS' processing framework. `Precomputing
   previews`_ in the processor documentation covers this in more detail;
   the upshot is that this can be as simple as::

     from gavo.helpers import processing

     class PreviewMaker(processing.SpectralPreviewMaker):
       sdmId = "build_sdm_data"

     if __name__=="__main__":
       processing.procmain(PreviewMaker, "flashheros/q", "import")



.. _precomputing previews: http://docs.g-vo.org/DaCHS/processors.html#precomputing-previews


Writing Custom Cores
====================

While DaCHS provides cores for many common operations -- in particular,
database queries and wrapped external binaries --, there are of course
services needing to do things not covered by what the shipped cores do.
Some such cases still follow the basic premise of services: GET or POST
parameters in, something table-like out.  For these cases, use custom
cores (if even this does not provide sufficent functionality, write a
custom renderer).


Defining a Custom Core
''''''''''''''''''''''

To do this, you need to write a python module.  The standard location
for those is in the bin/ subdirectory of the resource directory.

You will usually want to inherit from core::

  from gavo.svcs import core

  class Core(core.Core):

The framework will always look of an object named "Core" in the module
and use this as the custom core.

The core needs an InputTable and an OutputTable like all cores.  You
*could* define it in the resource descriptor like this::

  <customCore id="createCore" module="bin/create">
    <inputTable>
      <inputKey .../>
    </inputTable>
    <outputTable>
      <column name="itemsAdded" type="integer" tablehead="Items added"/>
    </outputTable>
  </customCore>

It's probably a better idea to define it in the code, though, since
then it will work without further specifications.  The definitions
in the code can still be overridden from an RD for special effects.
Embedding the definitions is done using the class attributes
``inputTableXML`` and ``outputTableXML``::

  class Core(core.Core):
    inputTableXML = """<inputTable>
      <inputKey name="fileSrc" type="file" tablehead="Local file"
        description="A local file to upload (overrides source URL if given).">
      <inputKey name="tableName" type="text" tablehead="Target Table"
        description="Name of the table to match against.  
          Only tables available for ADQL (see there) can be used here.">
        <values fromdb="tablename from dc_tables where adql=True"/>
      </inputTable>
      """
    outputTableXML = """<outputTable/>"""

You should not override the constructor.  If you need to perform
"expensive" instanciations, override the completeElement method, as in
the following template::

  def completeElement(self):
    <your code>
    self._completeElementNext(Core)

The call to _completeElementNext ensures that the remaining
completeElement methods are executed.

Giving the Core Functionality
'''''''''''''''''''''''''''''

To have the core do something, you have to override the run method,
which has to have the following signature::

  run(service, inputTable, queryMeta) -> stuff

The stuff returned will ususally be a Table instance (that need not
match the outputTable definition -- the latter is targetted at the
registry and possibly applications like output field selection).  The
standard renderers also accept a mime type and a string containing
some data and will deliver this as-is.  With custom renderers, you could
return basically anything you want.

Services come up with some idea of the schema of the table they want to
return and adapt tables coming out of the core to this.  Sometimes, you
want to suppress this behaviour, e.g., because the service's ideas are
off.  In that case, set a noPostprocess atttribute on the table to any
value.

service is a service instance.  In particular, you can access the RD you
are running in through its rd attribute.  This is useful if you need to
resolve, e.g., table references (which, in this case, could be given as
a service property)::

  pertainingTable = service.rd.getById(
    service.getProperty("pertainingTable"))

inputTable is a Table instance. Unless the service has a fancy inputDD,
you simply find the inputKey values in the table's parameters::

  val = inputTable.getParam("fileSrc")



Errors
''''''

To bail out from processing, raise a validation error.  Construct it
with a message and the name of an input key.  At least for the form
renderer, this causes a sensible error message with some hint as the the
originating input field::

  raise base.ValidationError("Invalid file name", "rdsrc")


Database Options
''''''''''''''''

The standard DB cores receive a "table widget" on form generation,
including sort and limit options.  To make the Form renderer output this
for your core as well, define a method wantsTableWidget() -> True.

The queryMeta that you receive in run has a dbLimit key.  It contains
the user selection or, as a fallback, the global db/defaultLimit value.
These values are integers.

So, if you order a table widget, you should do something like::

  cursor.execute("SELECT .... LIMIT %(queryLimit)s", 
    {"queryLimit": queryMeta["dbLimit"],...})

In general, you should warn people if the query limit was reached; a
simple way to do that is::

  if len(res)==queryLimit:
    res.addMeta("_warning", "The query limit was reached.  Increase it"
      " to retrieve more matches.  Note that unsorted truncated queries"
      " are not reproducible (i.e., might return a different result set"
      " at a later time).")

where res would be your result table.  _warning metadata is displayed in
both HTML and VOTable output, though of course VOTable tools will not
usually display it.

Inheriting from TableBasedCore
''''''''''''''''''''''''''''''

TBD (This does not work right now; complain if you need to do it)


Manufacturing Spectra
=====================

TODO: Update this for Datalink

Making SDM Tables
'''''''''''''''''

Compared to images, the formats situation with spectra is a mess.
Therefore, in all likelihood, you will need some sort of conversion
service to VOTables compliant to the spectral data model.  DaCHS has a
facility built in to support you with doing this on the fly, which means
you only need to keep a single set of files around while letting users
obtain the data in some format convenient to them.  The tutorial
contains examples on how to generate metadata records for such
additional formats.

First, you will have to define the "instance table", i.e., a table
definition that will contain a DC-internal representation of the
spectrum according to the data model.  There's a mixin for that::

  <table id="spectrum">
    <mixin ssaTable="hcdtest">//ssap#sdm-instance</mixin>
  </table>

In addition to adding lots and lots of params, the mixin also defines
two columns, ``spectral`` and ``flux``; these have units and ucds as
taken from the SSA metadata.  You can add additional columns (e.g., a
flux error depending the the spectral coordinate) as requried.

The actual spectral instances can be built by sdmCores and delivered
through DaCHS' product interface.  Note, however, that clients
`supporting getData`_ wouldn't need to do this.  You'll still have to
define the data item defined below.

sdmCores, while potentially useful with common services, are intended to
be used by the product renderer for dcc product table paths.  They
contain a data item that must yield a primary table that is basically
sdm compliant.  Most of this is done by the //ssap#feedSSAToSDM apply
proc, but obviously you need to yield the spectral/flux pairs (plus
potentially more stuff like errors, etc, if your spectrum table has more
columns.  This comes from the data item's grammar, which probably must
always be an embedded grammar, since its sourceToken is an SSA row in a
dictionary.  Here's an example::

  <sdmCore queriedTable="hcdtest" id="mksdm">
    <data id="getdata">
      <embeddedGrammar>
        <iterator>
          <code>
            labels = ("spectral", "flux")
            relPath = self.sourceToken["accref"].split("?")[-1]
            with self.grammar.rd.openRes(relPath) as inF:
              for ln in inF:
                yield dict(zip(labels,ln.split()))
          </code>
        </iterator>
      </embeddedGrammar>
      <make table="spectrum">
        <parmaker>
          <apply procDef="//ssap#feedSSAToSDM"/>
        </parmaker>
      </make>
    </data>
  </sdmCore>

Note: spectral, flux, and possibly further items coming out of the
iterator must be in the units units promised by the SSA metadata
(fluxSI, spectralSI).  Declarations to this effect are generated by the
``//ssap#sdm-instance`` mixin for the spectral and flux columns.

The sdmCores are always combined with the sdm renderer.  It passes an
accref into the core that gets turned into an row from queried table;
this must be an "ssa" table (i.e., right now something that mixes in
``//ssap#hcd``).  This row is the input to the embedded data descriptor.
Hence, this has no sources element, and you must have either a custom
or embedded grammar to deal with this input.


Echelle Spectra
===============

Echelle spectrographs "fold" a spectrum into several orders which may be
delivered in several independent mappings from spectral to flux
coordinate.  In this split form, they pose some extra problems, dealt
with in an extra system RD, ``//echelle``.  For merged Echelle spectra,
just use the standard SSA framework.


Table
'''''

Echelle spectra have additional metadata that should end up in their SSA
metadata table – these are things like the number of orders, the minimum
and maximum (Echelle) order, and the like.  To pull these columns into
your metadata table, use the ssacols stream, for example like this::

  <table id="ordersmeta" onDisk="True" adql="True">
    <meta name="description">SSA metadata for split-order 
      Flash/Heros Echelle spectra</meta>
    <mixin
      [...]
      statSpectError="0.05"
      spectralResolution="2.5e-11"
    >//ssap#hcd</mixin>
    <mixin
      calibLevel="1">//obscore#publishSSAPHCD</mixin>
    <column name="localKey" type="text"
      ucd="meta.id"
      tablehead="Key"
      description="Local observation key."
      verbLevel="1"/>
    <STREAM source="//echelle#ssacols"/>
  </table>



Supporting getData
==================

DaCHS still has support the now-abandoned 2012 getData specification by
Demleitner and Skoda.   If you think you still want this, contact the
authors; meanwhile, you really should be using datalink for whatever you
think you need getData for.



Adapting Obscore
================

You may want extra, locally-defined columns in your obscore tables.  To
support this, there are three hooks in obscore that you can exploit.
To fill these hooks, use ``userconfig.rd`` (TODO: more
documentation on that as we use it more; meanwhile: get a `template from
SVN`_ and put it into GAVO_ROOT/etc).  It helps to have a brief look at
the ``//obscore`` RD to get an idea where these hooks go.

Within the template ``userconfig.rd``, there are already three STREAMs
with ids starting with obscore.; these are referenced from within the
system ``//obscore`` RD.  Here's an somewhat more elaborate example::

  <STREAM id="obscore-extracolumns">
    <column name="fill_factor"
      description="Fill factor of the SED"
      verbLevel="20"/>
  </STREAM>

  <STREAM id="obscore-extrapars">
    <mixinPar name="fillFactor" 
      description="The SED's fill factor">NULL</mixinPar>
  </STREAM>

  <STREAM id="obscore-extraevents">
    <property name="obscoreClause" cumulate="True">
      ,
      CAST(\\\\fillFactor AS real) AS fill_factor,
    </property>
  </STREAM>

(to be on the safe side: there need to be four backslashes in front of
fillFactor; this is just a backslash doubly-escaped.  Sorry about this).

The way this is used in an actual mixin would be like this::

  <table id="specs" onDisk="True">
    <mixin ...>//ssap#hcd</mixin>
    <mixin
      ... (all the usual parameters)
      fillFactor="0.3">//obscore#publishSSAPHCD</mixin>
  </table>

What's going on here?  Well, ``obscore-extracolumns`` is easy – this
material is directly inserted into the definition of the obscore view
(see the table with id ``ObsCore`` within the ``//obscore`` RD).  You
could abuse it to insert other stuff than columns but probably should
not (current exception: you probably need to fix the ``viewStatement``
in //obscore to include sufficient columns; we're trying to figure out a
better solution).

The tricky part is ``obscore-extraevents``.  This goes into the
``//obscore#_publishCommon`` STREAM and ends up in all the publish
mixins in obscore.  Again, you could insert mixinPars and similar at
this point, but the only thing you really must do is add lines to the
big SQL fragment in the ``obscoreClause`` property that the mixin leaves
in the table.  This is what is made into the table's contribution to the
big obscore union. Just follow the example above and, in particular,
always CAST to the type you ave in the metadata, since individual tables
might have NULLs in the values, and you do not want misguided attempts
of postgres to do type inference then.

If you actually must know why you need to double-escape fillFactor and
what the magic with the ``cumulate="True"`` is, ask.

Finally, ``obscore-extrapars`` directly goes into a core component of
obscore, one that all the various publish mixins there use.  Hence, all
of them grow your functionality.  That is also why it is important to
give defaults (i.e., element content) to all mixinPars you give in this
way – without them, all those other publish mixins would fail.

If you change ``%#obscore-extracolumns``, you will need to re-import all
obscore-published tables (actually, importing the metadata using ``gavo
imp -m`` should do).  There currently is no automatic way to traverse
the file system, and you will probably have to first unpublish all
existing tables by connecting to the database and running ``delete from
ivoa._obscoresources``.  If obscore adaption proves a popular feature,
we'll make all this a bit smoother.

.. _template from SVN: http://svn.ari.uni-heidelberg.de/svn/gavo/python/trunk/gavo/resources/inputs/__system__/userconfig.rd





Writing Custom Grammars
=======================

A custom grammar simply is a python module located within a resource
directory defining a row iterator class derived from
gavo.grammars.customgrammar.CustomRowIterator; this class must be called
RowIterator.  You want to override the _iterRows method.  It will have
to yield row dictionaries, i.e., dictionaries mapping string keys to
something (preferably strings, but you will usually get away with
returning complete values even without fancy rowmakers).  

So, a custom grammar module could look like this::

  from gavo.grammars.customgrammar import CustomRowIterator

  class RowIterator(CustomRowIterator):
    def _iterRows(self):
      for i in xrange(10000):
        yield {'index': i, 'square': i**2}

Do not override magic methods, since you may lose row filters, sourceFields,
and the like if you do.  An exception is the constructor.  If you must,
you can override it, but you must call the parent constructor, like
this::

  class RowIterator(CustomRowIterator):
    def __init__(self, grammar, sourceToken, sourceRow=None):
      CustomRowIterator.__init__(self, grammar, sourceToken, sourceRow)
      <your code>

The sourceToken, in general, will be a file name, unless you call
makeData manually and forceSource something else.

A row iterator will be instanciated for each source processed.  Thus,
you should usually not perform expensive operations in the constructor
unless they depend on sourceToken.  In general, you should rather define
a function makeDataPack in the module.  Whatever is returned by this
function is available as self.grammar.dataPack in the row iterator.

The function receives an instance of the the customGrammar as an
argument.  This means you can access the resource descriptor and
properties of the grammar.  As an example of how this could be used,
consider this RD fragment::

  <table id="defTable">
    ...
  </table>

  <customGrammar module="res/grammar">
    <property name="targetTable">defTable</property>
  </customGrammar>

Then you could have the following in res/grammar.py::

  def makeDataPack(grammar):
    return grammar.rd.getById(grammar.getProperty("targetTable"))

and access the table in the row iterator.

Also look into EmbeddedGrammar, which may be a more convenient way to
achieve the same thing.

Dispatching Grammars
''''''''''''''''''''

With normal grammars, all rows are fed to all rowmakers of all makes
within a data object.  The rowmakers can then decide to not process a
given row by raising ``IgnoreThisRow`` or using the trigger mechanism.
However, when filling complex data models with potentially dozens of
tables, this becomes highly inefficient.

When you write your own grammars, you can to better.  Instead of just
yielding a row from ``_iterRows``, you yield a pair of a role (as
specified in the ``role`` attribute of a ``make`` element) and the row.
The machinery will then pass the row only to the feeder for the table in
the corresponding make.

Currently, the only way to define such a dispatching grammar is to use a
custom grammar or an embedded grammar.  For these, just change your
``_iterRows`` and say ``isDispatching="True"`` in the ``customGrammar``
element.  If you implement ``getParameters``, you can return either
pairs of role and row or just the row; in the latter case, the row will
be broadcast to all parmakers.

Special care needs to be taken when a dispatching grammar parses
products, because the product table is fed by a special make inserted
from the products mixin.  This make of course doesn't see the rows you
are yielding from your dispatching grammar.  This means that without
further action, your files will not end up the the product table at all.
In turn, getproducts will return 404s instead of your products.

To fix this, you need to explicitely yield the rows destined for the 
products table with a products role, from within your grammar.  Where
the grammar yield rows for the table with metadata (i.e., rows that actually
contain the fields with prodtblAccref, prodtblPath, etc), yield
to the products table, too, like this: ``yield ("products", newRow)``.


Functions Available for Row Makers
==================================

In principle, you can use arbitrary python expressions in var, map and
proc elements of row makers.  In particular, the namespace in which
these expressions are executed contains math, os, re, time, and datetime
modules as well as gavo.base, gavo.utils, and gavo.coords.

However, much of the time you will get by using the following functions
that are immediately accessible in the namespace:

.. replaceWithResult getRmkFuncs(docStructure)



Scripting
=========

As much as it is desirable to describe tables in a declarative manner,
there are quite a few cases in which some imperative code helps a lot
during table building or teardown.  Resource descriptors let you embed
such imperative code using script elements.  These are children of the
make elements since they are exclusively executed when actually
importing into a table.

Currently, you can enter scripts in SQL and python, which may be called
at various phases during the import.

SQL scripts
'''''''''''

In SQL scripts, you separate statements with semicolons.  Note that no
statements in an SQL script may fail since that will invalidate the
transaction.  This is a serious limitation since you must not commit or
begin transactions in SQL scripts as long as Postgres  does not support
nested transactions.

You can use table macros in the SQL scripts to parametrize them; the
most useful among those probably is ``\curtable`` containing the fully
qualified name of the table being processed.

Python scripts
''''''''''''''

Python scripts can be indented by a constant amount.

The table object currently processed is accessible as table.  In
particular, you can use this to issue queries using 
``table.query(query, arguments)`` (parallel to dbapi.execute) and to
delete rows using ``table.deleteMatching(condition, pars)``.  The
current RD is accessible as ``table.rd``, so you can access items from
the RD as ``table.rd.getById("some_id")``, and the recommended way to
read stuff from the resource directory is
``table.rd.openRes("res/some_file)``.

Some types of scripts may have additional names available.  Currently,
newSource and sourceDone have the name sourceToken – which is the
sourceToken as passed to the grammar.

Script types
''''''''''''

The type of a script corresponds to the event triggering its execution.
The following types are defined right now:

* preImport -- before anything is written to the table
* preIndex -- before the indices on the table are built
* postCreation -- after the table (incl. indices) is finished
* beforeDrop -- when the table is about to be dropped
* newSource -- every time a new source is started
* sourceDone -- every time a source has been processed

Note that preImport, preIndex, and postCreation scripts are not executed
when a table is updated, in particular, in data items with
``updating="True"``.  The only way to run scripts in such circumstances
is to use newSource and sourceDone scripts.


Examples
''''''''

This snippet sets a flag when importing some source (in this case,
that's an RD, so we can access sourceToken.sourceId::

      <script type="newSource" lang="python" id="markDeleted">
        table.query("UPDATE %s SET deleted=True"
          " WHERE sourceRD=%%(sourceRD)s"%id, 
          {"sourceRD": sourceToken.sourceId})
      </script>


This is a hacked way of ensuring some sort of referential integrity:
When a table containing "products" is dropped, the corresponding entries
in the products table are deleted::

  <script type="beforeDrop" lang="SQL" name="clean product table">
    DELETE FROM products WHERE sourceTable='\curtable'
  </script>

Note that this is actually quite hazardous because if the table is
dropped in any way not using the make element in the RD, this will not
be executed.  It's usually much smarter to tell the database to do the
housekeeping.  Rules are typically set in postCreation scripts::

  <script type="postCreation" lang="SQL">
    CREATE OR REPLACE RULE cleanupProducts AS 
      ON DELETE TO \curtable DO ALSO
      DELETE FROM products WHERE key=OLD.accref
  </script>

The decision if such arrangements are make before the import, before the
indexing or after the table is finished needs to be made based on the
script's purpose.

Another use for scripts is SQL function definition::

      <script type="postCreation" lang="SQL" name="Define USNOB matcher">
        CREATE OR REPLACE FUNCTION usnob_getmatch(alpha double precision, 
          delta double precision, windowSecs float
        ) RETURNS SETOF usnob.data AS $$
        DECLARE
          rec RECORD;
        BEGIN
          FOR rec IN (SELECT * FROM usnob.data WHERE 
            q3c_join(alpha, delta, raj2000, dej2000, windowSecs/3600.)) 
          LOOP
            RETURN NEXT rec;
          END LOOP;
        END;
        $$ LANGUAGE plpgsql;
      </script>

You can also load data, most usefully in preIndex scripts (although
beforeImport would work as well here)::

    <script type="preIndex" lang="SQL" name="create USNOB-PPMX crossmatch">
        SET work_mem=1000000;
        INSERT INTO usnob.ppmxcross (
          SELECT q3c_ang2ipix(raj2000, dej2000) AS ipix, p.localid 
          FROM 
            ppmx.data AS p, 
            usnob.data AS u 
          WHERE q3c_join(p.alphaFloat, p.deltaFloat, 
            u.raj2000, u.dej2000, 1.5/3600.))
    </script>



Bibliography
============

.. [RMI]  Hanisch, R., et al, "Resource Metadata for the Virtual
   Observatory", http://www.ivoa.net/Documents/latest/RM.html
.. [VOTSTC] Demleitner, M., Ochsenbein, F., McDowell, J., Rots, A.:
   "Referencing STC in VOTable", Version 2.0,
   http://www.ivoa.net/Documents/Notes/VOTableSTC/20100618/NOTE-VOTableSTC-2.0-20100618.pdf
.. _the DaCHS tutorial: http://docs.g-vo.org/DaCHS/tutorial.html
.. |date| date::
